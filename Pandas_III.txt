Научимся сортировать данные. Применять метод SORT.
Получить навыки группировки данных и анализа групп.
Освоить методы построения сводных таблиц и научиться с их помощью исследовать зависимости в признаках.
Научиться работать с данными, представленными в виде нескольких таблиц, и познакомиться с методами их объединения.
Обратите внимание, что наше преобразование столбцов к типам datetime и category «слетело» после нового считывания из csv. csv-файл не хранит в себе информацию о типах данных столбцов, поэтому при чтении Pandas автоматически определяет тип данных столбца.
МЕТОД SORT_VALUES().
Для сортировки значений в DataFrame по значениям одного или нескольких столбцов используется метод sort_values().
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html
Основные параметры метода sort_values():
by — имя или список имён столбцов, по значениям которых производится сортировка.
axis — ось, по которой производится сортировка (0 — строки, 1 — столбцы). По умолчанию сортировка производится по строкам.
ascending — сортировка по возрастанию (от меньшего к большему). По умолчанию параметр выставлен на True, для сортировки по убыванию (от большего к меньшему) необходимо выставить его на False.
ignore_index — создаются ли новые индексы в таблице. По умолчанию выставлен на False и сохраняет индексы изначальной таблицы.
inplace — производится ли замена исходной таблицы на отсортированную. По умолчанию параметр выставлен на False, то есть замены не производится. Чтобы переопределить исходную таблицу на отсортированную, необходимо выставить этот параметр на True.
СОРТИРОВКА ПО ЗНАЧЕНИЯМ ОДНОГО СТОЛБЦА.
melb_df.sort_values(by='Price').head(10) -  по возрастанию цены объектов недвижимости (Price).
СОРТИРОВКА ПО ЗНАЧЕНИЯМ НЕСКОЛЬКИХ СТОЛБЦОВ.
Для сортировки по значениям нескольких столбцов необходимо передать названия этих столбцов в параметр by в виде списка. При этом важно обращать внимание на порядок следования столбцов.
melb_df.sort_values(by=['Distance', 'Price']).loc[::10, ['Distance', 'Price']]- сначала по возрастанию расстояния от центра города (Distance), а затем — по возрастанию цены объекта (Price). Для нагляднjости, выделим каждую десятую строку из столбцов Distance и Price результирующей таблицы. Мы получили таблицу, отсортированную по возрастанию расстояния до центра города. Если встречаются объекты недвижимости, у которых расстояние оказывается одинаковым, то внутри такой группы производится сортировка по цене объекта.
КОМБИНИРОВАНИЕ СОРТИРОВКИ С ФИЛЬТРАЦИЕЙ.
Найдём информацию о таунхаусах (Type), проданных компанией (SellerG) McGrath, у которых коэффициент соотношения площадей здания и участка (AreaRatio) меньше -0.8. Результат отсортируем по дате продажи (Date) в порядке возрастания, а после проведём сортировку по убыванию коэффициента соотношения площадей. Также обновим старые индексы на новые, установив параметр ignore_index на True. Для наглядности результата выберем из таблицы только столбцы Data и AreaRatio:
mask1 = melb_df['AreaRatio'] < -0.8
mask2 = melb_df['Type'] == 'townhouse'
mask3 = melb_df['SellerG'] == 'McGrath'
melb_df[mask1 & mask2 & mask3].sort_values(
    by=['Date', 'AreaRatio'],
    ascending=[True, False],
    ignore_index=True
).loc[:, ['Date', 'AreaRatio']]
 Старайтесь не сочетать фильтрацию и метод sort_values() с параметром inplace=True, так как в таком случае у вас возникнет предупреждение (warning) SettingWithCopyWarning: melb_df[melb_df['Rooms'] > 5].sort_values(inplace=True, by=['Rooms']). вы пытаетесь напрямую изменить подмножество исходных данных. Чтобы не возникало подобных конфликтов, необходимо использовать метод copy() для явного создания копии отфильтрованного подмножества исходных данных и работать уже с ней (производить сортировку c inplace=True).
https://newtechaudit.ru/pandas-dataframes/ - об этом.
МЕТОД GROUPBY().
Одна из основных задач анализа данных — это группировка данных и сравнение показателей в группах. В некоторых случаях группировки может быть достаточно, чтобы ответить на вопросы бизнеса. В других случаях это может стать первым шагом в более сложном анализе. Так, например, благодаря группировке мы можем выявлять признаки, которые не несут статистической значимости, или признаки, которые вносят наибольший вклад.  Для группировки данных по одному или нескольким признакам можно использовать метод groupby(). https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html
Основные параметры метода groupby():
by — имя или список имён столбцов, по которым производится группировка.
axis — ось, по которой производится группировка (0 — строки, 1 — столбцы). По умолчанию группировка производится по строкам.
as_index — добавляется ли дополнительный индекс к таблице. По умолчанию установлен на True.
Метод groupby() возвращает объект DataFrameGroupBy, который хранит в себе информацию о том, какие строки относятся к определённой группе, и сам по себе не представляет для нас интереса. Однако к этому объекту можно применять уже знакомые нам агрегирующие методы (mean, median, sum и т. д.), чтобы рассчитывать показатели внутри каждой группы. Посмотрим на общую схему работы метода groupby() в Pandas: Сначала мы разделяем данные на группы с помощью метода groupby(), после чего к каждой группе применяем агрегацию и объединяем результаты в новую таблицу.
ГРУППИРОВКА ДАННЫХ ПО ОДНОМУ КРИТЕРИЮ С ОДНОЙ АГРЕГАЦИЕЙ.
melb_df.groupby(by='Type').mean(numeric_only=True)- Применим агрегирующую функцию среднего к результату работы groupby(). В качестве столбца для группировки возьмём столбец типа объекта недвижимости (Type). Мы получили таблицу, на пересечении строк и столбцов которой находятся средние значения каждого числового признака в наших данных. Обратите внимание на структуру получившейся таблицы: теперь на месте индексов стоят значения типа объекта недвижимости Type (house, townhouse, unit). Примечание. Если мы хотим видеть тип объекта в качестве отдельного столбца таблицы, мы можем выставить параметр as_index на False. Как правило, нам не нужна информация обо всех столбцах, поэтому агрегирующие методы можно применять только к интересующему нас столбцу. Например, давайте сравним средние цены на объекты в зависимости от их типа: melb_df.groupby('Type')['Price'].mean() Примечание. Обратите внимание, что, так как мы считаем только один показатель (среднее) для одного столбца, в результате мы получаем объект Series. Теперь давайте выясним, какие регионы (Regionname) наиболее удалены от центра Мельбурна. Для этого найдём минимальное значение расстояния от центра города до объекта в зависимости от его региона. Результат отсортируем по убыванию расстояния:
melb_df.groupby('Regionname')['Distance'].min().sort_values(ascending=False)
ГРУППИРОВКА ДАННЫХ ПО ОДНОМУ КРИТЕРИЮ С НЕСКОЛЬКИМИ АГРЕГАЦИЯМИ.
Чтобы рассчитать несколько агрегирующих методов, можно воспользоваться методом agg(), который принимает список строк с названиями агрегаций. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html 
Давайте построим таблицу для анализа продаж по месяцам. Для этого найдём количество продаж, а также среднее и максимальное значения цен объектов недвижимости (Price), сгруппированных по номеру месяца продажи (MonthSale). Результат отсортируем по количеству продаж в порядке убывания:
melb_df.groupby('MonthSale')['Price'].agg(
    ['count', 'mean', 'max']
).sort_values(by='count', ascending=False)
Примечание. Обратите внимание, что, так как мы считаем несколько показателей для одного столбца, в результате мы получаем объект DataFrame. В результате применения метода agg(), в который мы передали список с названиями интересующих нас агрегирующих функций, мы получаем DataFrame со столбцами count, mean и max, где для каждого месяца рассчитаны соответствующие параметры. Результат сортируем по столбцу count. Какие интересные выводы можно сделать из этой таблицы?
1.Пик продаж приходится на период весна-лето.
2.Средняя цена продаваемых объектов относительно стабильна и находится в пределах 1 млн. австралийских долларов с небольшими отклонениями (около 100 тыс. влево и вправо).
3.Прослеживается некоторая зависимость между сезоном и максимальной ценой объектов: в месяцы с большим спросом на объекты недвижимости цена также имеет наибольшие показатели. Можно сделать предположение, что это связано с повышением цен на элитные дома в периоды большого спроса. 
Примечание. Если вам нужна полная информация обо всех основных статистических характеристиках внутри каждой группы, вы можете воспользоваться методом agg(), передав в качестве его параметра строку 'describe':
melb_df.groupby('MonthSale')['Price'].agg('describe')
После базовых математических функций наиболее частым агрегированием является подсчёт числа уникальных значений. Так, например, мы можем вычислить число уникальных риелторских компаний в зависимости от региона, чтобы понять, в каких регионах конкуренция на рынке недвижимости меньше. Это можно сделать, передав в параметр метода agg() строку 'nunique'. 
Более того, метод agg() поддерживает использование и других функций. Передадим дополнительно встроенную функцию set, чтобы получить множество из агентств недвижимости, которые работают в каждом из регионов:
melb_df.groupby('Regionname')['SellerG'].agg(
    		['nunique', set]
Методы группировки по одному признаку позволяют посмотреть на наши данные «в разрезе» группирующего признака и понять их взаимосвязь. Далее мы рассмотрим, как оценивать взаимосвязь между большим количеством признаков.
Предлагаем вам самим потренироваться в использовании группировки 
СВОДНЫЕ ТАБЛИЦЫ.
Это распространённый инструмент для агрегации данных. Сводная таблица принимает на вход данные из отдельных столбцов и группирует их. В результате получается новая таблица, которая позволяет увидеть многомерное обобщение данных. Таким образом, благодаря сводным таблицам мы можем оценить зависимость между двумя и более признаками данных. Мы чаще сталкиваемся со сводными таблицами, чем с обычными, в плоском виде (Под «плоским видом» подразумевается, что индексами являются номера строк, а столбцами — имена столбцов.), так как сводные таблицы удобнее для анализа и быстрых выводов, а также позволяют увидеть более общие зависимости между признаками, нежели простая группировка данных.
МЕТОД GROUPBY КАК СПОСОБ ПОСТРОЕНИЯ СВОДНЫХ ТАБЛИЦ. 
простейшие одномерные сводные таблицы с помощью метода groupby — мы рассматривали, по сути это сводная таблица в контексте группировки по одному признаку. groupby для более сложных группировок melb_df.groupby(['Rooms', 'Type'])[['Price', 'BuildingArea']].mean()- группируем сначала по Rooms, потом по Type и выводим для анализа средние признаков 'Price' 'BuildingArea' для сгруппированных данных. В результате выполнения такого кода мы получаем DataFrame, который обладает несколькими уровнями индексов: первый уровень — число комнат, второй уровень — тип здания. Такая организация индексов называется иерархической. Вычисление параметра (средней цены) и площади сдания, происходит во всех возможных комбинациях признаков. Если бы вместо ['Price', 'BuildingArea'] была бы 'Price'- мы получили бы Serias с теми же уровнями индексов.
mlb_df.groupby(['Rooms', 'Type'])['Price'].mean().unstack() - unstack(), Для того, чтобы финальный результат был представлен в виде сводной таблицы (тип DataFrame) (первый группировочный признак по строкам, а второй — по столбцам), а не в виде Series с иерархическими индексами, к результату чаще всего применяют метод unstack(), который позволяет переопределить вложенный индекс в виде столбцов таблицы. В результате мы получаем сводную таблицу, столбцы в которой представляют типы домов (house, townhouse, unit), строки — число комнат, а на пересечении строк и столбцов находится средняя стоимость объекта с такими показателями.
mlb_df.groupby(['Rooms', 'Type'])[['Price', 'BuildingArea']].mean().unstack()- даст тоже DataFrame,  но с более сложной, разветвленной структурой столбцов. То есть сначала столбцы с средней ценой, в зависмости от к-ва комнат, потом с средней площадью здания, в зависимости от того же типа.
МЕТОД PIVOT_TABLE ДЛЯ ПОСТРОЕНИЯ СВОДНЫХ ТАБЛИЦ.
На самом деле метод groupby редко используется при двух параметрах группировки, так как для построения сводных таблиц существует специальный и более простой метод — pivot_table().
Основные параметры метода pivot_table()
values — имя столбца, по которому необходимо получить сводные данные, применяя агрегирующую функцию;
index — имя столбца, значения которого станут строками сводной таблицы;
columns — имя столбца, значения которого станут столбцами сводной таблицы;
aggfunc — имя или список имён агрегирующих функций (по умолчанию — подсчёт среднего, 'mean');
fill_value — значение, которым необходимо заполнить пропуски (по умолчанию пропуски не заполняются).
Та же самая таблицу, но уже с использованием метода pivot_table. В качестве параметра values укажем столбец Price, в качестве индексов сводной таблицы возьмём Rooms, а в качестве столбцов — Type. Агрегирующую функцию оставим по умолчанию (среднее). Дополнительно заменим пропуски в таблице на значение 0. Финальный результат для наглядности вывода округлим с помощью метода round() до целых. Несложно понять, что метод pivot_table() имеет преимущество перед группировкой по нескольким критериям. Оно заключается в наличии специальных аргументов для строк и столбцов сводной таблицы, благодаря чему уменьшается вероятность запутаться при построении более сложных (многомерных) сводных таблиц, о которых мы поговорим далее.
Разберём ещё один пример: найдём, как зависит средняя и медианная площадь участка (Landsize) от типа объекта (Type) и его региона (Regionname). Чтобы посмотреть несколько статистических параметров, нужно передать в аргумент aggfunc список из агрегирующих функций. Построим такую сводную таблицу, где пропущенные значения заменим на 0:
melb_df.pivot_table(
    values='Landsize',
    index='Regionname',
    columns='Type',
    aggfunc=['median', 'mean'],
    fill_value=0
)
Разберём ещё один пример: найдём, как зависит средняя и медианная площадь участка (Landsize) от типа объекта (Type) и его региона (Regionname). Чтобы посмотреть несколько статистических параметров, нужно передать в аргумент aggfunc список из агрегирующих функций. Построим такую сводную таблицу, где пропущенные значения заменим на 0:
melb_df.pivot_table(
    values='Landsize',
    index='Regionname',
    columns='Type',
    aggfunc=['median', 'mean'],
    fill_value=0
)
Обратите внимание на добавление дополнительных индексов столбцов median и mean. Здесь медианное и среднее значения рассчитаны отдельно для каждой комбинации признаков.
МНОГОМЕРНЫЕ СВОДНЫЕ ТАБЛИЦЫ.
До этого мы рассматривали, как некоторый статистический показатель(медиана цены) может зависеть от двух признаков(размера участка и региона). Однако, как уже упоминалось, сводные таблицы позволяют наблюдать зависимость и от большего числа признаков. Такие сводные таблицы называются многомерными. Для того чтобы исследовать зависимость от большего числа признаков, можно передать список признаков в параметр index или параметр columns.
ДОСТУП К ДАННЫМ В СВОДНОЙ ТАБЛИЦЕ.
Пусть есть сводная табдица:
pivot = melb_df.pivot_table(
    values='Landsize',
    index='Regionname',
    columns='Type',
    aggfunc=['median', 'mean'],
    fill_value=0
)
Тогда можно вывести структуру её столбцов: pivot.columns
В результате мы получаем объект MultiIndex. Этот объект хранит в себе шесть комбинаций пар столбцов (два статистических параметра и три типа здания), то есть есть шесть возможных вариантов обращения к столбцам таблицы.
Мультииндексы раскрываются подобно вложенным словарям — по очереди, как матрёшка. Чтобы получить доступ к определённому столбцу, вы должны сначала обратиться к столбцу, который находится уровнем выше.
Так, из таблицы pivot мы можем получить срединные значения площадей участков для типа здания unit, просто последовательно обратившись по имени столбцов:
display(pivot['median']['unit'])
Regionname
Eastern Metropolitan          203.0
Eastern Victoria              230.0
Northern Metropolitan           0.0
Northern Victoria               0.0
South-Eastern Metropolitan    199.0
Southern Metropolitan           0.0
Western Metropolitan           62.0
Western Victoria                0.0
Name: unit, dtype: float64
Воэможно дальнейшая детализация: pivot['mean']['unit']['Western Metropolitan']
557.63723150358
Аналогично производится и фильтрация данных, pivot таблица это DataFrame, но специального вида. Например, если нам нужны регионы, в которых средняя площадь здания для домов типа house меньше их медианной площади, то мы можем найти их следующим образом:
mask1 = pivot['mean']['house'] < pivot['median']['house']
pivot[mask1]
                                 median	                        mean
Type	                         house	townhouse	unit	house	townhouse	unit
Regionname						
Southern Metropolitan	586.0	246.0	0.0	569.643881	278.858824	466.380245
Western Metropolitan	531.0	198.0	62.0	507.883406	244.560669	557.637232
Чтобы получить индексы отфильтрованной таблицы, можно воспользоваться атрибутом index и обернуть результат в список:
print(list(filtered_pivot.index))
Таким образом, сводные таблицы изначально кажутся сложной структурой, но на самом деле это обычные DataFrame со вложенными индексами строк или столбцов.  Умение читать и анализировать сложные сводные таблицы — это важный навык, который помогает проводить углублённый анализ данных.
Примечание. На самом деле мультииндексные таблицы можно создавать и вручную. Давайте посмотрим на синтаксис данной конструкции:
import numpy as np
mser = pd.Series(
    np.random.rand(8),
	index=[['white','white','white','blue','blue','red','red','red'], 
           ['up','down','right','up','down','up','down','left']])
display(mser)
white  up       0.904321
       down     0.245350
       right    0.106912
blue   up       0.542156
       down     0.520282
red    up       0.517427
       down     0.281742
       left     0.051510
dtype: float64
В данном примере мы создаём объект Series со вложенными индексами. Мы передаём в качестве индексов Series вложенный список, где первый список задаёт внешний уровень вложенности, а второй список — внутренний уровень вложенности. Значения Series — случайные числа от 0 до 1, сгенерированные функцией np.random.rand() (ваши значения могут отличаться).
Если посмотреть на индексы Series, можно увидеть, что они являются мультииндексами:
MultiIndex([('white',    'up'),
            ('white',  'down'),
            ('white', 'right'),
            ( 'blue',    'up'),
            ( 'blue',  'down'),
            (  'red',    'up'),
            (  'red',  'down'),
            (  'red',  'left')],
           )
Аналогично создаются DataFrame со вложенными признаками (вложенными столбцами) — для этого вложенный список передаётся в параметр columns при инициализации таблицы:
mframe = pd.DataFrame(
    np.random.randn(16).reshape(4,4),
    index=[['white','white','red','red'], ['up','down','up','down']],
    columns=[['pen','pen','paper','paper'],[1,2,1,2]]
)
					pen				paper
  						1		2		1		2
white	up				-0.002400	-0.680597	0.396839	0.433390
	down				1.850197	0.080564	1.051687	1.800390
red	up				1.453534	-0.701890	0.799755	1.523138
	down				0.469531	1.717623	-0.406269	-2.87078
ОБЪЕДИНЕНИЕ DataFrame.
С КАКИМИ ДАННЫМИ МЫ РАБОТАЕМ? ratings1 и ratings2 — таблицы с данными о выставленных пользователями оценках фильмов. Они имеют одинаковую структуру и типы данных — на самом деле это две части одной таблицы с оценками фильмов.
dates — таблица с датами выставления всех оценок. movies — таблица с информацией о фильмах1
1.Склеим таблицы ratings1 и ratings2 в единую структуру.
2.Термин «склеить» в данном случае обозначает конкатенацию — присоединение одной таблицы к другой.
3.К полученной таблице с рейтингами подсоединим столбец с датой проставления рейтинга, склеив столбцы таблиц между собой.
4.Присоединим к нашей таблице информацию о названиях и жанрах фильмов.
ЗАЧЕМ ХРАНИТЬ ДАННЫЕ В РАЗНЫХ ТАБЛИЦАХ?
Конечно, здорово, если все необходимые данные лежат в одной таблице, но на практике такое случается редко по двум объективным причинам: Часто данные формируются несколькими независимыми процессами, каждый из которых хранит данные в своей таблице. Например, данные для отчёта по продажам могут состоять из списка банковских транзакций, курсов валют от Центробанка и планов отдела продаж из внутренней CRM. Все эти три таблицы, скорее всего, будут формироваться независимыми друг от друга системами. Объединять их в один отчёт придётся вам. Хранить все данные в одной таблице часто очень накладно для мкости диска. Например, названия фильмов в наших данных хранятся в отдельной небольшой таблице. А в логах, которые могут растягиваться на многие миллионы строк, вместо названия фильма стоит его идентификатор. Числовой идентификатор фильма занимает на диске гораздо меньше места, чем длинное название, поэтому логи с идентификаторами будут занимать гораздо меньше места, чем единая таблица с названиями.
-СКЛЕИТЬ ТАБДИЦЫ ratings1 и ratings2 как по строкам  так и по столбцам:  функция concat()
objs -список объектов DataFrame ([df1, df2,…]), которые должны быть сконкатенированы;
axis — ось определяет направление конкатенации: 0 — конкатенация по строкам (по умолчанию), 1 — конкатенация по столбцам;
join — либо inner (пересечение), либо outer (объединение); рассмотрим этот момент немного позже;
ignore_index — по умолчанию установлено значение False, которое позволяет значениям индекса оставаться такими, какими они были в исходных данных. Если установлено значение True, параметр будет игнорировать исходные значения и повторно назначать значения индекса в последовательном порядке.
Для корректной конкатенации по строкам объединяемые таблицы должны иметь одинаковую структуру — идентичное число и имена столбцов. Итак, давайте склеим  ratings1 и ratings2 по строкам, так как они имеют одинаковую структуру столбцов. Для этого передадим их списком в функцию concat(). Помним, что параметр axis по умолчанию равен 0, объединение происходит по строкам, поэтому не трогаем его. Примечание. Обратите внимание, что concat является функцией библиотеки, а не методом DataFrame. Поэтому её вызов осуществляется как pd.concat(...). ratings = pd.concat([ratings1, ratings2]) В результате мы увеличили первую таблицу, добавив снизу строки второй таблицы. Узнаем количество строк в таблицах ratings и dates, ведь нам предстоит вертикально склеить их между собой. ОЧИСТИТЬ таблицу от дублей, мы можем воспользоваться методом DataFrame drop_duplicates(), который удаляет повторяющиеся строки в таблице. Не забываем обновить индексы после удаления дублей, выставив параметр ignore_index в методе drop_duplicates() на значение True.
 Теперь можем добавить к нашей таблице с оценками даты их выставления. Для этого конкатенируем таблицы ratings и dates по столбцам: ratings_dates = pd.concat([ratings, dates], axis=1)
Объединение DataFrame: join, merge
ОБЪЕДИНЕНИЕ DataFrame: join, merge.
У таблиц ratings и movies есть общий столбец movieId(общий признак), который каждому фильму из таблицы movies ставит в соответствие поставленные ему оценки из таблицы ratings. Мы хотим объединить их в единую структуру согласно этому соответствию. Объединения такого рода часто называют объединением по ключевому столбцу.
ТИПЫ ОБЪЕДИНЕНИЙ.
Типы объединений в Pandas тесно связаны с операцией join из SQL, которую мы будем рассматривать в курсе в дальнейшем. 
Существует два основных типа объединения таблиц: inner (внутреннее); outer (внешнее).
inner (внутреннее). При использовании такого типа объединения в результирующей таблице остаются только те записи, которые есть в обеих таблицах(у которых есть совпадающие общие признаки). Аналогия в теории множеств: Пересечение (intersection) множеств А и В.. Строки, для которых совпадение не было найдено, удаляются.
outer (внешнее). Данный тип делится на три подтипа:
full — используется как outer по умолчанию, объединяет все варианты в обеих таблицах. Аналогия в теории множеств. Объединение (union) множеств А и В.
left — для всех записей из «левой» таблицы (например, ratings) ведётся поиск соответствий в «правой» (например, movies). В результирующей таблице останутся только те значения, которым были найдены соответствия, то есть только значения из ratings.
Аналогия в теории множеств: Вычитание (difference) множества B из результата объединения (union) множеств А и В.
right — аналогично предыдущему, но остаются значения только из «правой» таблицы.  Аналогия в теории множеств: Вычитание (difference) множества А из результата объединения (union) множеств А и В.
Во всех трёх случаях, если совпадений между таблицами не найдено, на этом месте ставится пропуск (NaN).

810375162512091  brest syabrovskogo 105 kv.90 Hatnyanskiy
МЕТОД ОБЪЕДИНЕНИЯ JOIN
joined_false = ratings_dates.join(
    movies,
    rsuffix='_right',
    how='left'
) - объединяет по индексам (записи с одинаковыми индексами), к одинаковым названиям столбцов добовляет соответствующие суффиксы. Берет левый df к соответсвующим по индексу записям добавляет данные из правого df  в столбцы правого df. Там где соответствие не найдено добовляется NaN. Не получаем соответствия фильмов и их рейтингов.
Для соответствия - необходимо использовать ключевой столбец в «правой» таблице в качестве индекса. Это можно сделать с помощью метода set_index() в правом df. Также необходимо указать название ключа в параметре on.
joined = ratings_dates.join(
    movies.set_index('movieId'),
    on='movieId',
    how='left'
) То есть теперь объединение не по индексам, а по ключевому столбцу (moveId). В результате такого объединения для каждого идентификатора фильма movieId в таблице ratings_dates найден совпадающий с ним идентификатор movieId в таблице movies и присоединена информация о самом фильме (title и genres). Это как раз то, что нам нужно. Обратите внимание, что в результате такого объединения остался лишь один столбец movieId.
МЕТОД ОБЪЕДИНЕНИЯ MERGE.
Аналогично предыдущему, метод merge() предназначен для слияния двух таблиц по ключевым столбцам или по индексам. Однако, в отличие от join(), метод merge() предлагает более гибкий способ управления объединением, благодаря чему является более популярным.
Основные параметры метода merge(). 
right — присоединяемая таблица. По умолчанию она является «правой».
how — параметр типа объединения. По умолчанию принимает значение 'inner'.
on — параметр, который определяет, по какому столбцу происходит объединение. Определяется автоматически, но рекомендуется указывать вручную.
left_on — если названия столбцов в «левой» и «правой» таблицах не совпадают, то данный параметр отвечает за наименования ключевого столбца исходной таблицы.
right_on — аналогично предыдущему, параметр отвечает за наименование ключевого столбца присоединяемой таблицы.
Метод merge() в первую очередь предназначен для слияния таблиц по заданным ключам, поэтому он не требует установки ключевых столбцов в качестве индекса присоединяемой таблицы. Кроме того, данный метод позволяет объединять даже таблицы с разноимёнными ключами. Таким образом, merge() проще в использовании и более многофункционален, чем схожие методы.
merged = ratings_dates.merge(
    movies,
    on='movieId',
    how='left'
) - merged df == joined df из последнего примера объединения join по ключевому столбцу moveId, но не надо устанавливать ключевой столбец moveId как индекс.
ОСОБЕННОСТИ ИСПОЛЬЗОВАНИЯ MERGE().
Важно! Учитывайте такие нюансы при работе с несколькими таблицами и всегда проверяйте результат объединения. То есть проверять количество строк начальных df и результата. Если результат больше, чем самамя длинная из начальных df. Нада искать значения NaN и решать, что с ними делать.
Метод merge() с внешним (outer) типом объединения может использоваться как аналог метода concat() при объединении таблиц с одинаковой структурой (одинаковые количество и названия столбцов) по строкам. В таком случае все одноимённые столбцы таблиц будут считаться ключевыми.
merge_ratings = ratings1.merge(ratings2, how='outer')
print('Число строк в таблице merge_ratings: ', merge_ratings.shape[0])
display(merge_ratings)
# Число строк в таблице merge_ratings: 100836. При использовании метода merge() для склейки двух таблиц у нас автоматически пропали дубликаты, которые мы видели при использовании метода concat(). Это особенность метода merge() — автоматическое удаление дублей.
КАКОЙ МЕТОД ОБЪЕДИНЕНИЯ ИСПОЛЬЗОВАТЬ?
См. картинку ConcatJoinMerge.png
https://tproger.ru/translations/regular-expression-python  - python работа с регулярными выражениями.
https://pythobyte.com/how-to-merge-dataframes-in-pandas-23614/- как правильно объединять
